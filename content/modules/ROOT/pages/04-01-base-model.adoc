= Deploy and Test the Base Model
include::_attributes.adoc[]

== Deploy the Base Model

To deploy a model you need to go to the created Data Science Project (`{user}`) and:

* Click on **Models** and select the **Single-model service platform** option
+
[.bordershadow]
image::04/04-01-single-model.png[]
* Click on **Deploy model**
+
[.bordershadow]
image::04/04-01-deploy.png[]
* And fill the form with the required information:
** Name: `base`
** Serving runtime: `vLLM ServingRuntime for KServe`
** Model server size: `Small`
** Accelerator: `NVIDIA GPU`
** Model route: select option to make model available through external route
** Token authentication: select `Require token authetication` and leave the default **Service account name**
** Existing connection:
*** Connection: `Minio - models`
*** Path: `base_model`
+
[.bordershadow]
image::04/04-01-model-inputs.png[]
* Click on **Deploy** and wait for the model to be ready
+
[.bordershadow]
image::04/04-01-model-status.png[]


== Test the Base Model

Now that the model is deployed, you can test it by sending a request to the model, and also checking the time it takes to respond.

=== Environment Setup

. Clone the repository ({git-clone-repo-url}) in your laptop and go to the folder with the lab material for this section (`neural-magic-workshop/lab-materials/04`)
. Update the following variables in `request.py`:
+
[source,python]
----
MODEL = "your-model-name"  // Replace with your model name
URL = "your-api-url"       // Replace with your API endpoint
API_KEY = "your-api-key"   // Replace with your API key
----

To get the information from the deployed model to fill up those vars, you should use:

* The Model name (`base`) for the MODEL
* Copy the `token` for the API_KEY
* Check the `internal and external endpoints details` of the deployed model for the URL. Use the **external endpoint**.
+
[.bordershadow]
image::04/04-01-inference-endpoints.png[]

=== Installation Steps

Participants should ensure that Git and Python are installed and ready to use on their laptop. Then:

. Create and activate a Python virtual environment:
+
[source,bash]
----
python -m venv venv
source venv/bin/activate
----
. Install the required dependency:
+
[source,bash]
----
pip install langchain_openai
----

=== Running the Script

To run the script and measure execution time:
[source,bash]
----
time python request.py
----

This will execute the script and display:

* The script's output
* Real time (wall clock time)
* User CPU time
* System CPU time

=== Remove model

To remove the model simply click on the **Delete** on the **Models** tab

IMPORTANT: You need to remove the model before going to the next step in order to have enough GPUs.
[.bordershadow]
image::04/04-01-model-delete.png[]
[.bordershadow]
image::04/04-01-model-delete-base.png[]
